
In order to observe the effect of number of iterations, modify the variable 
num_iters when calling the init() functions


Linear regression 

getBeta uses closed form, formula: beta = (X^T*X)^ -1)*(X^T*Y)

getBetaBatchGradient uses formula:
gradient = sum over i…n( x_i * x_iT * beta - y_i)/n
updates beta using: beta(new) = beta(old) - gradient

getBetaStochasticGradient
performs stochastic gradient descent to find optimal beta using formula
            loss = y_i -x_iT*(beta)
            change = x_i*loss
            beta(new) = beta(old) + lr * change

Sigmoid1 is the sigmoid function, designed to prevent overflow in the case of extremely large or small x values.


getBeta_BatchGradient implements batch gradient descent, 
calculates gradient using formula:
xT * (pi - y)
where pi is sigmoid(train_x*Beta)


getBeta_BatchGradientReg implements the regularization specified, and takes 
in an additional parameter gamma, which is the regularization term constant.
We use the new loss function, and take the gradient accordingly using it to update beta the same way as before.

Gradient is: dL/d(beta_j) = sum over i..n (x_ij * (y_i -p(x_i, Beta)))/n + gamma * Beta_j


getBeta_Newton calculates the Hessian matrix using the formula:
H = xT * (diag(p_i * (1 - p_i)) * x
where p_i is the i’th value of sigmoid(train_x*Beta)

Updates Beta using Beta(new) = Beta(old)  - H^-1 * gradient